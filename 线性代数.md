# 线性代数

## 第一章 线性方程组

### 1.1 线性方程组

![image-20240418142319798](img/image-20240418142319798.png)

### 1.2 行化简与阶梯型矩阵

![image-20240418142553731](img/image-20240418142553731.png)

![image-20240418143034646](img/image-20240418143034646.png)

什么是初等矩阵：

![image-20240423150649209](img/image-20240423150649209.png)

### 1.3 向量方程

![image-20240418143335383](img/image-20240418143335383.png)

### 1.4 矩阵方程

![image-20240418144258501](img/image-20240418144258501.png)

### 1.5 方程组的解集

![image-20240418154103044](img/image-20240418154103044.png)

### 1.6 线性方程组的应用

略

### 1.7 线性无关

![image-20240418155132240](img/image-20240418155132240.png)

### 1.8 线性变换介绍

![image-20240418155903621](img/image-20240418155903621.png)

### 1.9 线性变换的矩阵

![image-20240418160850568](img/image-20240418160850568.png)

### 1.10 商业和工程中的线性模型

略

## 第二章 矩阵代数

### 2.1 矩阵运算

比较简单，包括有

1. 加减乘、乘幂，乘又分为标量乘和矩阵乘法
2. 转置

### 2.2  矩阵的逆

![image-20240422101441308](img/image-20240422101441308.png)

![image-20240422101414469](img/image-20240422101414469.png)

### 2.3 可逆矩阵的性质

![image-20240422101816665](img/image-20240422101816665.png)

### 2.4 分块矩阵

我理解分块矩阵是为了适配计算机有限的内存、分而治之的加速方法，但我没有太掌握。

记录一条定理草草了事：

![image-20240422103306053](img/image-20240422103306053.png)



### 2.5 矩阵因式分解

#### 2.5.1 LU分解

![image-20240422112906605](img/image-20240422112906605.png)

![image-20240422112131295](img/image-20240422112131295.png)

一个霸蛮算的例子（第一列的a b d本来可以一开始就填上的，为原矩阵的第一列除以第一个元素值）

![image-20240419090943041](img/image-20240419090943041.png)

**LU分解的算法，如果遇到需要做行交换的场景，应该怎么操作？**

chatgpt说：记录每次行交换的顺序，在最后据此恢复原始矩阵的顺序

### 2.6 Leotief投入产出模型

我没有认真看这一节

### 2.7 计算机图形学中的应用

我没有认真看这一节

### 2.8 Rⁿ 子空间

![image-20240422114102594](img/image-20240422114102594.png)

### 2.9 维数和秩

![image-20240422115135275](img/image-20240422115135275.png)

## 第三章 行列式

### 3.1 行列式的定义

![image-20240423141402352](img/image-20240423141402352.png)

### 3.2 行列式的性质

![image-20240423141807973](img/image-20240423141807973.png)

### 3.3 克拉默法则、体积和线性变换

![image-20240423142450145](img/image-20240423142450145.png)

## 第四章 向量空间

### 4.1 - 4.6

本章的前面几节内容和第三章的7、8、9节的内容比较相似，只补充一些没有明确提到的内容：

![image-20240424091306756](img/image-20240424091306756.png)



![image-20240424095744261](img/image-20240424095744261.png)



![image-20240424104328634](img/image-20240424104328634.png)

### 4.7 坐标变换

![image-20240424131733937](img/image-20240424131733937.png)

### 4.8 向量空间在差分方程（不是微分方程）和马尔科夫链的应用

我没有看，看不太懂。应该不会考

## 第五章 特征值和特征向量

### 5.1 特征值和特征向量

![image-20240425174707742](img/image-20240425174707742.png)

求矩阵{{8,0},{0,2}}的特征向量，很容易得出特征向量是{0,0}的错误结论，要特别小心。仔细的去做，就会发现是{1,0 }和{0, 1}。 （这是Mathematica的表达方式）

### 5.2 特征方程

![image-20240425175249902](img/image-20240425175249902.png)

### 5.3 对角化

![image-20240425180356048](img/image-20240425180356048.png)

![image-20240425180324904](img/image-20240425180324904.png)

### 5.4 特征向量与线性变换 --5.7节

这几节我居然看不懂看不懂看不懂

### 5.8 特征值的迭代估计

土话就是:

把一个可对角化的方阵（存在特征值）不断的乘以一个向量，最终获得的向量满足：

1. 其方向，会趋近绝对值最大的特征值所对应的特征向量所在直线的方向。
2. 其大小，会趋近c1x lamda^k x v1

![image-20240426173810247](img/image-20240426173810247.png)

迭代估计特征值的办法：

![image-20240428143403443](img/image-20240428143403443.png)

上面的看不太懂，感觉没有那么复杂，就是不断的计算A的幂乘以x0  (x0要选择最大分量为1)， 当k比较大的时候，用 (A^k)x0 除以 (A^(k-1))x0，得到一个近似值就是A的特征值。附上Mathematica代码和输出：

```mathematica
ClearAll["Global`*"];  (*这个函数可诡异了，一定要这样写才是符合预期的，其他不带参数或者怎样的通通不对，日他妈的*)
A = {{6, 7}, {8, 5}};
x = {1, 0};
For[i = 0, i < 5, ++i, Print[A.x]; oldx = x; x = A.x;];
Print[N[LeastSquares[ Transpose[{oldx}], x]]];

{13.0005}
```

手工计算A的特征值也可以发现是13和-2

疑惑点：x0满足最大分量是1也是ok的，例如{2,3}，可能只是为了让手工做上述计算的时候工作量小一点吧。

上面计算过程会迷惑懵懂的我：是不是所有的方阵都有这样的性质？  不断的乘一个向量，最后趋近于数乘这个向量？ 当然不是，只有存在特征值的方阵才可以。

## 第六章 正交性和最小二乘法

### 6.1 内积、长度和正交性

这一节主要是向量的基本概念。最重要的概念是向量的内积（点积）。

![image-20240429154607457](img/image-20240429154607457.png)

### 6.2 正交集

![image-20240429160516994](img/image-20240429160516994.png)

### 6.3 正交投影

![image-20240429161057945](img/image-20240429161057945.png)

### 6.4 格拉姆-施密特方法（构造正交基的算法）

![image-20240429162700819](img/image-20240429162700819.png)

### 6.5 最小二乘问题

![image-20240429164947169](img/image-20240429164947169.png)

### 6.6 最小二乘的应用

![image-20240429170006560](img/image-20240429170006560.png)

### 6.7 6.8内积空间和及其应用

我理解内积空间是前面内积算子的进一步泛化，我先不学了

## 第七章 对称矩阵和二次型

### 7.1 对称矩阵

土话就是：对称矩阵不只能对角化，还能正交对角化，且能摆出一个特殊样式的“谱”。

![image-20240430142518510](img/image-20240430142518510.png)

### 7.2 二次型

![image-20240430144703558](img/image-20240430144703558.png)

### 7.3 条件优化

这一节的内容不是很直观，平时我比较少用到这样的优化问题。

![image-20240503105431797](img/image-20240503105431797.png)

### 7.4 奇异值分解

![image-20240503105921264](img/image-20240503105921264.png)

![image-20240503110215349](img/image-20240503110215349.png)

奇异值分解的算法：

![image-20240503112907766](img/image-20240503112907766.png)

[奇异值分解的实际用途](https://github.com/bisonliao/goodgoodstudy/blob/master/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0.md#%E5%A5%87%E5%BC%82%E5%80%BC%E5%88%86%E8%A7%A3%E7%9A%84%E5%AE%9E%E9%99%85%E7%94%A8%E9%80%94)

1. 图像压缩
2. 主成分分析、数据降维
3. 对图像提炼特征，用于图像识别/比对



```mathematica
ClearAll["Global`*"];
A = {{-18, 13, -4, 4}, {2, 19, -4, 12}, {-14, 11, -12, 8}, {-2, 21, 4, 8}};
{u, w, v} = SingularValueDecomposition[A];
Print["SingularValue:", u, w, v];

svdCompress[A_, qnum_] :=  Module[{u, w, v, result, m, n, i, j, u1, v1, q}, 
   {u, w, v} =  SingularValueDecomposition[A];
   m = Length[A];(*图片高*)
   n = Length[A[[1]]];(*图片宽*)
   Print["m:", m , ",n:", n];
   q = qnum;
   If[q > Min[m, n], q = Min[m, n], null];
   
   (*  m x n的0矩阵作为初始画布  *)
   result = Table[ConstantArray[0, n], {i, 1, m}];
   
   Print["datasize:", q*(m + n)];
   Print["singular values:", Table[w[[i, i]], {i, 1, q}]];
   Do[
    u1 = Table[{u[[j, i]]}, {j, 1, m}];(* m x 1 , 第i个特征值对应的u向量 *)
    v1 = {v[[;; , i]]}; (* 1 x n，第i个特征值对应的v向量 *)
    
    result = result + (u1.v1)*w[[i, i]];,  (* 
    u.v得到m x n的一个矩阵，是第i个特征值对应的分量*)
    {i, 1, q}(*这样的特征值取前面q个，后面的分量抛弃，实现压缩*)];
   
   result];
   
svdCompress[A, 1] (* 原矩阵的第一个近似值*)
svdCompress[A, 2] (* 原矩阵的第二个近似值*)
svdCompress[A, 3] (* 原矩阵的第三个近似值*)
```



### 7.5 图像处理与统计学中的应用



主成分分析：

![image-20240503142731514](img/image-20240503142731514.png)

```mathematica
ClearAll["Global`*"];
X = {{120, 125, 125, 135, 145}, {61, 60, 64, 68, 72}};  (*学生身高和体重的采样数据，5个样本。X的第二行这个特征明显与第一行很相关*)
S = {{100, 47.5}, {47.5, 25}}; (*手工算出协方差矩阵*)
Print["eigenvalues:", Eigenvalues[S]];
U = Eigenvectors[S];
Y = U.X;(*会看到Y的第二行绝对值很小，可以忽略。第一行才是主成分*)
Print[U];
Print["Y:", Y];
Print[Transpose[U].Y];(*逆运算从Y恢复X*)

(*尝试忽略次要成分，看看恢复出来的X是什么样子*)
YY = {Y[[1, ;;]]};
YY = Join[YY, {{0, 0, 0, 0, 0}}];
Print[YY];
Print[Transpose[U].YY];
```

